{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "extracted_data=load_pdf_file(data='../data_rp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(extracted_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocessing: \n",
      "Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval\n",
      "Luyu Gao1, Xueguang Ma2, Jimmy Lin2, and Jamie Callan1\n",
      "1Language Technologies Institute\n",
      "Carnegie Mellon University\n",
      "2David R. Cheriton Sc\n",
      "\n",
      "After Preprocessing: \n",
      "Tevatron An Efficient Flexible Toolkit Dense Retrieval Luyu Gao1 Xueguang Ma2 Jimmy Lin2 Jamie Callan1 1Language Technologies Institute Carnegie Mellon University 2David R Cheriton School Computer Sci\n",
      "Before Preprocessing: \n",
      "The positive targets are usually human judged and the negative\n",
      "texts are usually non-relevant texts from top results of a baseline\n",
      "retrieval system such as BM25. The second format (not shown due\n",
      "{\n",
      "\"qu\n",
      "\n",
      "After Preprocessing: \n",
      "The positive targets usually human judged negative texts usually nonrelevant texts top results baseline retrieval system BM25 The second format shown due queryid query id query query text positivepass\n",
      "Before Preprocessing: \n",
      "library [11] as our retriever’s backend. It implements several ef-\n",
      "ficient indices in C++ and exposes them through Python inter-\n",
      "faces. For users who want the best performance, Tevatron pro-\n",
      "vides a s\n",
      "\n",
      "After Preprocessing: \n",
      "library 11 retrievers backend It implements several ef ficient indices C exposes Python inter faces For users want best performance Tevatron pro vides simple class BaseFaissIPRetriever wraps flat fais\n",
      "Before Preprocessing: \n",
      "RAM GPU memory Time\n",
      "DPR-repo 60G 20G x 4 2.0 hours\n",
      "Tevatron-default 17G 17G x 4 1.5 hours\n",
      "Tevatron-GradCache 4 G 15G x 1 7.0 hours\n",
      "Tevatron-TPU 10G – 1.0 hours\n",
      "Table 2: Training efficiency comparison \n",
      "\n",
      "After Preprocessing: \n",
      "RAM GPU memory Time DPRrepo 60G 20G x 4 20 hours Tevatrondefault 17G 17G x 4 15 hours TevatronGradCache 4 G 15G x 1 70 hours TevatronTPU 10G 10 hours Table 2 Training efficiency comparison original DP\n",
      "Before Preprocessing: \n",
      "generalizable functionalities provide IR community convenience in\n",
      "future dense retrieval research.\n",
      "ACKNOWLEDGE\n",
      "We would like to thank Google’s TPU Research Cloud (TRC) for\n",
      "access to Cloud TPUs and Com\n",
      "\n",
      "After Preprocessing: \n",
      "generalizable functionalities provide IR community convenience future dense retrieval research ACKNOWLEDGE We would like thank Googles TPU Research Cloud TRC access Cloud TPUs Compute Canada access GP\n"
     ]
    }
   ],
   "source": [
    "# Function to clean text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by:\n",
    "    - Removing non-alphanumeric characters (except spaces).\n",
    "    - Normalizing whitespace.\n",
    "    - Removing headers, footers, or page numbers using regex patterns.\n",
    "    \"\"\"\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    # print(text)\n",
    "    print(\"Before Preprocessing: \")\n",
    "    print(text[:200])\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Normalize whitespace (remove extra spaces, tabs, and newlines)\n",
    "\n",
    "    # Remove page numbers or headers/footers (example pattern)\n",
    "    text = re.sub(r\"Page \\d+|Header Text|Footer Text\", \"\", text)\n",
    "    # print(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, stop_words)) + r')\\b'\n",
    "\n",
    "    result = re.sub(pattern, '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "    print()\n",
    "    print(\"After Preprocessing: \")\n",
    "    print(text[:200])\n",
    "    return text\n",
    "\n",
    "# Process each page and clean its content\n",
    "cleaned_pages = []\n",
    "for page in extracted_data:\n",
    "    if page.page_content:  # Ensure the page is not blank\n",
    "        # print(page.page_content[:200])\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "        # print(page.page_content[:200])\n",
    "\n",
    "# Print cleaned content for verification\n",
    "# for cleaned_page in cleaned_pages:\n",
    "#     print(f\"Page {cleaned_page['page_number']}:\\n{cleaned_page['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 27\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def section_based_chunking(extracted_data):\n",
    "    chunks = []\n",
    "    \n",
    "    # Pattern to identify section headers in academic papers\n",
    "    section_pattern = re.compile(r'\\n\\d+[\\.\\s]+[A-Z][\\w\\s]+\\n|^\\d+[\\.\\s]+[A-Z][\\w\\s]+\\n|\\n[A-Z][A-Z\\s]+\\n')\n",
    "    \n",
    "    for doc in extracted_data:\n",
    "        text = doc.page_content\n",
    "        \n",
    "        # Find all section boundaries\n",
    "        section_matches = list(section_pattern.finditer(text))\n",
    "        \n",
    "        if not section_matches:\n",
    "            # If no sections found, use default chunking\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "            doc_chunks = splitter.split_text(text)\n",
    "        else:\n",
    "            # Split by identified sections\n",
    "            doc_chunks = []\n",
    "            for i in range(len(section_matches)):\n",
    "                start = section_matches[i].start()\n",
    "                end = section_matches[i+1].start() if i < len(section_matches)-1 else len(text)\n",
    "                section_text = text[start:end]\n",
    "                doc_chunks.append(section_text)\n",
    "        \n",
    "        # Create document objects with metadata\n",
    "        for chunk in doc_chunks:\n",
    "            chunks.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "text_chunks=section_based_chunking(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 122\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "def text_split_markdown(extracted_data):\n",
    "    markdown_splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    \n",
    "    # Convert documents to markdown format if needed\n",
    "    markdown_chunks = []\n",
    "    for doc in extracted_data:\n",
    "        # Process each document\n",
    "        chunks = markdown_splitter.create_documents([doc.page_content])\n",
    "        # Preserve metadata\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = doc.metadata\n",
    "        markdown_chunks.extend(chunks)\n",
    "    \n",
    "    return markdown_chunks\n",
    "text_chunks=text_split_markdown(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the Data into Text Chunks\n",
    "# def text_split(extracted_data):\n",
    "#     text_splitter=RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "#     text_chunks=text_splitter.split_documents(extracted_data)\n",
    "#     return text_chunks\n",
    "\n",
    "# text_chunks=text_split(extracted_data)\n",
    "# print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Chunks Generated: \n",
      "------------------------\n",
      "Chunk-0: \n",
      "page_content='\n",
      "ABSTRACT\n",
      "Recent rapid advancements in deep pre-trained language models\n",
      "and the introductions of large datasets have powered research in\n",
      "embedding-based dense retrieval. While several good research pa-\n",
      "pers have emerged, many of them come with their own software\n",
      "stacks. These stacks are typically optimized for some particular re-\n",
      "search goals instead of efficiency or code structure. In this paper, we\n",
      "present Tevatron, a dense retrieval toolkit optimized for efficiency,\n",
      "flexibility, and code simplicity. Tevatron provides a standardized\n",
      "pipeline for dense retrieval including text processing, model train-\n",
      "ing, corpus/query encoding, and search. This paper presents an\n",
      "overview of Tevatron and demonstrates its effectiveness and effi-\n",
      "ciency across several IR and QA data sets. We also show how Teva-\n",
      "tron’s flexible design enables easy generalization across datasets,\n",
      "model architectures, and accelerator platforms(GPU/TPU). We be-\n",
      "lieve Tevatron can serve as an effective software foundation for\n",
      "dense retrieval system research including design, modeling, and\n",
      "optimization.' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with acmart 2018/07/25 v1.55 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-03-14T00:37:09+00:00', 'author': 'Luyu Gao1, Xueguang Ma2, Jimmy Lin2, and Jamie Callan1', 'keywords': '', 'moddate': '2022-03-14T00:37:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': 'Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval', 'trapped': '/False', 'source': '../data_rp/RP1.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk-1: \n",
      "page_content='\n",
      "1 INTRODUCTION\n",
      "Dense retrieval’s popularity in the research community has greatly\n",
      "grown in the past years [8, 13, 16, 22, 26]. By modeling relevance\n",
      "with query-document vector products, dense retrievers can carry\n",
      "out efficient and effective semantic search.\n",
      "While the idea of vector-based search is not new, the adoption of\n",
      "deep pre-trained language models as encoder [7] has substantially\n",
      "boosted the effectiveness of dense retrieval [13]. Meanwhile, similar\n",
      "to other research that relies on deep learning, the success of dense\n",
      "retrieval will not be possible without large data. Many of recent\n",
      "research works are based on their own software with specialized\n",
      "support only for specific datasets and models [13, 26]. We however\n",
      "believe flexible generalization across models and datasets is critical.\n",
      "Tevatron provides researchers with access to the latest state-of-\n",
      "the-art models and makes it easy for them to start a new research\n",
      "problem on a new dataset.\n",
      "In our past research on dense retrieval [8, 9, 18], we have run\n",
      "into several engineering challenges specific to dense systems. For\n",
      "example, in terms of resources, large corpora and training sets\n",
      "require large CPU memory; accelerator (GPU/TPU) memory usage\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "Conference’17, July 2017, Washington, DC, USA\n",
      "© 2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\n",
      "https://doi.org/10.1145/nnnnnnn.nnnnnnn\n",
      "also grows with model size. While orthogonal to actual research,\n",
      "these engineering problems slow down and constrain researchers,\n",
      "especially those with limited hardware resources. With Tevatron,\n",
      "we aim at providing a unified solution to common engineering\n",
      "problems.\n",
      "Tevatron incorporates several popular widely-used open-source\n",
      "packages, including datasets [15], transformers [25] and FAISS [11]\n",
      "respectively as backbone for our data management, neural network\n",
      "modeling and embedding-based retrieval components.\n",
      "To accommodate different research needs, we select two deep\n",
      "learning frameworks for Tevatron, Pytorch [21] and JAX[4]. Py-\n",
      "torch’s eager execution patterns and intuitive object-oriented de-\n",
      "sign have gained its massive user base in the research community.\n",
      "On the other hand, JAX, backed by just-in-time (JIT) XLA com-\n",
      "pilation, offers smooth transitions across hardware stacks with\n",
      "optimized performance.\n",
      "The rest of the paper is organized as follows. Section 2 gives an\n",
      "overview of Tevatron. Section 3 demonstrates Tevatron usage and\n",
      "command-line interface. Section 4 shows the experimental results\n",
      "of running Tevatron with various models and datasets.' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with acmart 2018/07/25 v1.55 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-03-14T00:37:09+00:00', 'author': 'Luyu Gao1, Xueguang Ma2, Jimmy Lin2, and Jamie Callan1', 'keywords': '', 'moddate': '2022-03-14T00:37:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': 'Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval', 'trapped': '/False', 'source': '../data_rp/RP1.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk-2: \n",
      "page_content='\n",
      "2 TOOLKIT OVERVIEW\n",
      "Tevatron1 is packaged as a Python module available on the Python\n",
      "Package Index. Tevatron can be installed via pip, as follows:\n",
      "$ pip install tevatron==0.1.0\n",
      "In this section, we give an overview of the core components of\n",
      "Tevatron. We demonstrate how these components respectively sup-\n",
      "port the full pipeline of data preparation, training, encoding, and\n",
      "search. Code and documentation of Tevatron are available at its\n",
      "website, tevatron.ai.\n",
      "2.1 Data Management\n",
      "Having data ready to use is a critical preliminary step before train-\n",
      "ing or encoding starts. Data access overhead and constraints could\n",
      "directly affect training/encoding performance. In Tevatron, we\n",
      "adopt the following core design: 1) text data are pre-tokenized be-\n",
      "fore training or encoding happens, 2) keep tokenized data memory-\n",
      "mapped instead of lazy-loaded or in-memory. The former avoids\n",
      "overheads when running sub-word/piece level tokenizers and also\n",
      "reduces data traffic compared to raw text. The latter allows random\n",
      "data access in the training/encoding loop without consuming a\n",
      "large amount of physical memory.\n",
      "Tevatron defines two basic raw input format templates for IR\n",
      "and QA context. As shown in Fig. 1, for the IR dataset (e.g. MS\n",
      "MARCO [2]), we organize a training instance into an anchor query,\n",
      "a list of positive target texts, and a list of negative target texts.\n",
      "1http://tevatron.ai\n",
      "arXiv:2203.05765v1  [cs.IR]  11 Mar 2022' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with acmart 2018/07/25 v1.55 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-03-14T00:37:09+00:00', 'author': 'Luyu Gao1, Xueguang Ma2, Jimmy Lin2, and Jamie Callan1', 'keywords': '', 'moddate': '2022-03-14T00:37:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': 'Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval', 'trapped': '/False', 'source': '../data_rp/RP1.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Text Chunks Generated: \")\n",
    "print(\"------------------------\")\n",
    "print(\"Chunk-0: \")\n",
    "print(text_chunks[0])\n",
    "print()\n",
    "print(\"Chunk-1: \")\n",
    "print(text_chunks[1])\n",
    "print()\n",
    "print(\"Chunk-2: \")\n",
    "print(text_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='HFQueryDataset and HFCorpusDataset for encoding) which will\n",
      "perform fast parallel data formatting and tokenization. Processed\n",
      "data is internally represented as a datasets.Dataset object and\n",
      "is stored in Apache Arrow format which can be memory-mapped\n",
      "and randomly accessed by offset.' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with acmart 2018/07/25 v1.55 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-03-14T00:37:09+00:00', 'author': 'Luyu Gao1, Xueguang Ma2, Jimmy Lin2, and Jamie Callan1', 'keywords': '', 'moddate': '2022-03-14T00:37:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': 'Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval', 'trapped': '/False', 'source': '../data_rp/RP1.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocessing: \n",
      "Command-line interface. Section 4 shows the experimental results of running Tevatron with various models and datasets. 2 TOOLKIT OVERVIEW Tevatron is packaged as a Python module available on the Python Package Index. Tevatron can be installed via pip, as follows: \\$ pip install tevatron==0.1.0\n",
      "\n",
      "After Preprocessing: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/yr/ccrg_j914ln8wydhc308crs00000gp/T/ipykernel_28948/1705238954.py:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  clean_text(\"Command-line interface. Section 4 shows the experimental results of running Tevatron with various models and datasets. 2 TOOLKIT OVERVIEW Tevatron is packaged as a Python module available on the Python Package Index. Tevatron can be installed via pip, as follows: \\$ pip install tevatron==0.1.0\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Commandline interface Section 4 shows experimental results running Tevatron various models datasets 2 TOOLKIT OVERVIEW Tevatron packaged Python module available Python Package Index Tevatron installed via pip follows pip install tevatron010'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for t in text_chunks[:3]:\n",
    "#     print(clean_text(t.page_content))\n",
    "#     print(\"----------------------------\")\n",
    "#     print()\n",
    "\n",
    "clean_text(\"Command-line interface. Section 4 shows the experimental results of running Tevatron with various models and datasets. 2 TOOLKIT OVERVIEW Tevatron is packaged as a Python module available on the Python Package Index. Tevatron can be installed via pip, as follows: \\$ pip install tevatron==0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/ccrg_j914ln8wydhc308crs00000gp/T/ipykernel_28948/3587992455.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "/Users/AjaysPC1/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447726368904114, 0.03102319873869419, 0.006734949070960283, 0.0261089988052845, -0.03936203196644783, -0.16030246019363403, 0.06692399084568024, -0.0064414506778120995, -0.047450508922338486, 0.014758870005607605, 0.07087535411119461, 0.05552756413817406, 0.01919330097734928, -0.02625131793320179, -0.010109501890838146, -0.026940520852804184, 0.022307423874735832, -0.022226650267839432, -0.14969263970851898, -0.01749308779835701, 0.007676230277866125, 0.054352328181266785, 0.003254461335018277, 0.031725965440273285, -0.08462142199277878, -0.029405977576971054, 0.05159567669034004, 0.0481240376830101, -0.003314798232167959, -0.05827917903661728, 0.04196927323937416, 0.022210724651813507, 0.12818880379199982, -0.022338908165693283, -0.011656265705823898, 0.06292837113142014, -0.032876305282115936, -0.09122606366872787, -0.031175317242741585, 0.052699554711580276, 0.04703483358025551, -0.0842030718922615, -0.030056171119213104, -0.020744813606142998, 0.009517783299088478, -0.003721737302839756, 0.007343312259763479, 0.039324332028627396, 0.09327404946088791, -0.003788584843277931, -0.05274207517504692, -0.05805819109082222, -0.0068643828853964806, 0.005283231846988201, 0.082893006503582, 0.019362758845090866, 0.006284475326538086, -0.010330792516469955, 0.00903236772865057, -0.03768376633524895, -0.04520608112215996, 0.024016335606575012, -0.006944148801267147, 0.013491622172296047, 0.10005496442317963, -0.0716838464140892, -0.021695023402571678, 0.03161848708987236, -0.05163462832570076, -0.08224770426750183, -0.06569331139326096, -0.009895365685224533, 0.005816418211907148, 0.07355456054210663, -0.034050293266773224, 0.02488609589636326, 0.01448806095868349, 0.02645735815167427, 0.009656740352511406, 0.030217353254556656, 0.052803974598646164, -0.07535990327596664, 0.00989719107747078, 0.029836835339665413, 0.01755557581782341, 0.023091964423656464, 0.001933867926709354, 0.0014001803938299417, -0.047175969928503036, -0.011194352991878986, -0.1142013669013977, -0.019811993464827538, 0.0402662418782711, 0.002193023217841983, -0.07979223877191544, -0.02538231946527958, 0.09448296576738358, -0.02898111194372177, -0.14500249922275543, 0.23097741603851318, 0.02773113362491131, 0.03211143612861633, 0.03106500767171383, 0.04283282533288002, 0.0642378181219101, 0.032163169234991074, -0.004876736551523209, 0.055699463933706284, -0.037532370537519455, -0.021505579352378845, -0.0283427182585001, -0.028846891596913338, 0.038353100419044495, -0.017468642443418503, 0.052485279738903046, -0.07487605512142181, -0.03125973418354988, 0.021841518580913544, -0.0398956835269928, -0.008587084710597992, 0.026956621557474136, -0.04849550873041153, 0.011469908989965916, 0.0296182781457901, -0.02057219110429287, 0.013103877194225788, 0.028833428397774696, -3.194200554190157e-33, 0.0647820234298706, -0.018130220472812653, 0.05178995802998543, 0.12198272347450256, 0.028780153021216393, 0.008722013793885708, -0.07052116096019745, -0.016907289624214172, 0.040739722549915314, 0.0421161949634552, 0.02544720284640789, 0.035746220499277115, -0.04914478212594986, 0.002129094675183296, -0.015546608716249466, 0.05073055997490883, -0.04818528890609741, 0.03588062524795532, -0.0040670339949429035, 0.10172472894191742, -0.05597005784511566, -0.010681056417524815, 0.011235825717449188, 0.09068648517131805, 0.004234469961374998, 0.03513865917921066, -0.00970282033085823, -0.09386519342660904, 0.0928555577993393, 0.008004953153431416, -0.007705444470047951, -0.052086733281612396, -0.01258796639740467, 0.0032669170759618282, 0.006013509351760149, 0.0075816200114786625, 0.010517175309360027, -0.08634550124406815, -0.06987877190113068, -0.0025338525883853436, -0.09097658842802048, 0.04688732698559761, 0.052076492458581924, 0.0071938661858439445, 0.010903649032115936, -0.005229500588029623, 0.013937331736087799, 0.02196834236383438, 0.03420855477452278, 0.06022468954324722, 0.00011667161743389443, 0.01473195105791092, -0.07008922845125198, 0.028499029576778412, -0.02760162204504013, 0.010768415406346321, 0.0348309688270092, -0.022487875074148178, 0.009769059717655182, 0.07722782343626022, 0.02158837392926216, 0.11495622247457504, -0.0680011585354805, 0.023760998621582985, -0.015983954071998596, -0.017826972529292107, 0.06439493596553802, 0.03202575445175171, 0.050270263105630875, -0.005913743749260902, -0.03370801359415054, 0.01784036122262478, 0.016573378816246986, 0.06329655647277832, 0.0346771776676178, 0.046473417431116104, 0.0979061871767044, -0.006635480094701052, 0.025207024067640305, -0.07798832654953003, 0.016926415264606476, -0.0009458136628381908, 0.022471889853477478, -0.038253266364336014, 0.09570480138063431, -0.005350737366825342, 0.01046905480325222, -0.11524057388305664, -0.013262508437037468, -0.010709413327276707, -0.08311724662780762, 0.07327359169721603, 0.04939219728112221, -0.008994407020509243, -0.09584559500217438, 3.366148929092564e-33, 0.12493181973695755, 0.019349683076143265, -0.05822574347257614, -0.0359882228076458, -0.05074677616357803, -0.045662395656108856, -0.08260344713926315, 0.14819474518299103, -0.08842118829488754, 0.06027449667453766, 0.05103021487593651, 0.01030312106013298, 0.14121422171592712, 0.030813826248049736, 0.061033111065626144, -0.05285125970840454, 0.13664889335632324, 0.00918987113982439, -0.017325248569250107, -0.012848636135458946, -0.007995273917913437, -0.05098007991909981, -0.052350617945194244, 0.007593035232275724, -0.015166294761002064, 0.016960332170128822, 0.021270543336868286, 0.020557992160320282, -0.12002810090780258, 0.0144618209451437, 0.026759885251522064, 0.02533061057329178, -0.042754657566547394, 0.006768374238163233, -0.014458597637712955, 0.04526199400424957, -0.09147657454013824, -0.019439158961176872, -0.017833473160862923, -0.05491010472178459, -0.052641090005636215, -0.010459025390446186, -0.05201610550284386, 0.020892055705189705, -0.07997037470340729, -0.012111334130167961, -0.057731397449970245, 0.02317824773490429, -0.008031624369323254, -0.025989271700382233, -0.07995668053627014, -0.02072882279753685, 0.04881777986884117, -0.020389163866639137, -0.04917660355567932, 0.014159666374325752, -0.06362209469079971, -0.007807407062500715, 0.016431517899036407, -0.025682535022497177, 0.013381090015172958, 0.02624872513115406, 0.009978359565138817, 0.06322892755270004, 0.002672085538506508, -0.0065828049555420876, 0.01663198135793209, 0.03236641734838486, 0.03794250264763832, -0.03637603297829628, -0.006910939700901508, 0.0001596680813236162, -0.0016335678519681096, -0.027278173714876175, -0.028038019314408302, 0.04968152195215225, -0.02886716090142727, -0.0024180228356271982, 0.014774885028600693, 0.009764573536813259, 0.005797567777335644, 0.013486114330589771, 0.005567923653870821, 0.03722706437110901, 0.007232501171529293, 0.04015624150633812, 0.08150327205657959, 0.07199162989854813, -0.013056167401373386, -0.04288209229707718, -0.011011275462806225, 0.004897747654467821, -0.009229715913534164, 0.03519148379564285, -0.05103500559926033, -1.571437557856825e-08, -0.08862441033124924, 0.02390933223068714, -0.01623873971402645, 0.03170048072934151, 0.027284206822514534, 0.05246880277991295, -0.047070976346731186, -0.05884743481874466, -0.06320824474096298, 0.04088851064443588, 0.0498279333114624, 0.10655167698860168, -0.07450230419635773, -0.012495414353907108, 0.01837068609893322, 0.03947409987449646, -0.024797892197966576, 0.014516244642436504, -0.037069205194711685, 0.020015733316540718, -4.859723412664607e-05, 0.009866524487733841, 0.024838799610733986, -0.05245812609791756, 0.029314152896404266, -0.08719191700220108, -0.014499811455607414, 0.026019059121608734, -0.018746402114629745, -0.07620514184236526, 0.03504328429698944, 0.10363951325416565, -0.028050530701875687, 0.01271818857640028, -0.07632554322481155, -0.01865236461162567, 0.024976715445518494, 0.0814453586935997, 0.06875890493392944, -0.06405662000179291, -0.08389390259981155, 0.0613623782992363, -0.03354554995894432, -0.10615336894989014, -0.04008057713508606, 0.032530199736356735, 0.07662484794855118, -0.07301613688468933, 0.0003376144450157881, -0.040871575474739075, -0.07578855007886887, 0.027527663856744766, 0.07462543994188309, 0.017717313021421432, 0.09121843427419662, 0.11022018641233444, 0.0005698199383914471, 0.051463332027196884, -0.01455132570117712, 0.03323201462626457, 0.02379225194454193, -0.022889820858836174, 0.038937538862228394, 0.03020678646862507]\n"
     ]
    }
   ],
   "source": [
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY2')\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ForbiddenException",
     "evalue": "(403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-07', 'x-cloud-trace-context': '93a4ec6fa1120030981f2c7d27930539', 'date': 'Fri, 28 Mar 2025 20:39:05 GMT', 'server': 'Google Frontend', 'Content-Length': '196', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"FORBIDDEN\",\"message\":\"Request failed. You've reached the max serverless indexes allowed in project Default (5). To add more serverless indexes, upgrade your plan.\"},\"status\":403}\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mForbiddenException\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m pc \u001B[38;5;241m=\u001B[39m Pinecone(api_key\u001B[38;5;241m=\u001B[39mPINECONE_API_KEY)\n\u001B[1;32m      6\u001B[0m index_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresearch-paper-llm-db5\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mpc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdimension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m384\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcosine\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mServerlessSpec\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcloud\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maws\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mregion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mus-east-1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m)\u001B[49m \n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/control/pinecone.py:384\u001B[0m, in \u001B[0;36mPinecone.create_index\u001B[0;34m(self, name, dimension, spec, metric, timeout, deletion_protection)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspec must be of type dict, ServerlessSpec, or PodSpec\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 384\u001B[0m \u001B[43mapi_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_index_request\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateIndexRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdimension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdimension\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_spec\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdeletion_protection\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_ready\u001B[39m():\n\u001B[1;32m    395\u001B[0m     status \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_status(name)\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:761\u001B[0m, in \u001B[0;36mEndpoint.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    751\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This method is invoked when endpoints are called\u001B[39;00m\n\u001B[1;32m    752\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[1;32m    753\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    759\u001B[0m \n\u001B[1;32m    760\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 761\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/control/api/manage_indexes_api.py:273\u001B[0m, in \u001B[0;36mManageIndexesApi.__init__.<locals>.__create_index\u001B[0;34m(self, create_index_request, **kwargs)\u001B[0m\n\u001B[1;32m    271\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_host_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_host_index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    272\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreate_index_request\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m create_index_request\n\u001B[0;32m--> 273\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_with_http_info\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:819\u001B[0m, in \u001B[0;36mEndpoint.call_with_http_info\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    816\u001B[0m     header_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_client\u001B[38;5;241m.\u001B[39mselect_header_content_type(content_type_headers_list)\n\u001B[1;32m    817\u001B[0m     params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m header_list\n\u001B[0;32m--> 819\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_api\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    820\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mendpoint_path\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    821\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttp_method\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    823\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    824\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbody\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpost_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth_settings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43masync_req\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43masync_req\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_check_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_check_return_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_return_http_data_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_return_http_data_only\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_preload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_preload_content\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_request_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_request_timeout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    835\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_formats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcollection_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:380\u001B[0m, in \u001B[0;36mApiClient.call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001B[39;00m\n\u001B[1;32m    327\u001B[0m \n\u001B[1;32m    328\u001B[0m \u001B[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;124;03m    then the method will return the response directly.\u001B[39;00m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m async_req:\n\u001B[0;32m--> 380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__call_api\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheader_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mauth_settings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_return_http_data_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcollection_formats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_preload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_request_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_check_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool\u001B[38;5;241m.\u001B[39mapply_async(\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__call_api,\n\u001B[1;32m    401\u001B[0m     (\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    418\u001B[0m     ),\n\u001B[1;32m    419\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:187\u001B[0m, in \u001B[0;36mApiClient.__call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PineconeApiException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    186\u001B[0m     e\u001B[38;5;241m.\u001B[39mbody \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39mbody\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_response \u001B[38;5;241m=\u001B[39m response_data\n\u001B[1;32m    191\u001B[0m return_data \u001B[38;5;241m=\u001B[39m response_data\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:175\u001B[0m, in \u001B[0;36mApiClient.__call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001B[0m\n\u001B[1;32m    171\u001B[0m     url \u001B[38;5;241m=\u001B[39m _host \u001B[38;5;241m+\u001B[39m resource_path\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# perform request and return response\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     response_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheader_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpost_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_preload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_preload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_request_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_request_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PineconeApiException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    186\u001B[0m     e\u001B[38;5;241m.\u001B[39mbody \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39mbody\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/api_client.py:460\u001B[0m, in \u001B[0;36mApiClient.request\u001B[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrest_client\u001B[38;5;241m.\u001B[39mOPTIONS(\n\u001B[1;32m    451\u001B[0m         url,\n\u001B[1;32m    452\u001B[0m         query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    457\u001B[0m         body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    458\u001B[0m     )\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrest_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPOST\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpost_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_preload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_preload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_request_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_request_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPUT\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrest_client\u001B[38;5;241m.\u001B[39mPUT(\n\u001B[1;32m    471\u001B[0m         url,\n\u001B[1;32m    472\u001B[0m         query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    477\u001B[0m         body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    478\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/rest.py:345\u001B[0m, in \u001B[0;36mRESTClientObject.POST\u001B[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mPOST\u001B[39m(\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    337\u001B[0m     url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    343\u001B[0m     _request_timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    344\u001B[0m ):\n\u001B[0;32m--> 345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpost_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_preload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_preload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_request_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_request_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LangChain_Projects/research_paperbot/mchatbot_venv/lib/python3.12/site-packages/pinecone/core/openapi/shared/rest.py:271\u001B[0m, in \u001B[0;36mRESTClientObject.request\u001B[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UnauthorizedException(http_resp\u001B[38;5;241m=\u001B[39mr)\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[0;32m--> 271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ForbiddenException(http_resp\u001B[38;5;241m=\u001B[39mr)\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m404\u001B[39m:\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFoundException(http_resp\u001B[38;5;241m=\u001B[39mr)\n",
      "\u001B[0;31mForbiddenException\u001B[0m: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-07', 'x-cloud-trace-context': '93a4ec6fa1120030981f2c7d27930539', 'date': 'Fri, 28 Mar 2025 20:39:05 GMT', 'server': 'Google Frontend', 'Content-Length': '196', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"FORBIDDEN\",\"message\":\"Request failed. You've reached the max serverless indexes allowed in project Default (5). To add more serverless indexes, upgrade your plan.\"},\"status\":403}\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"notebook-paper-llm-db5\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x31cbe6bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Existing index \n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='be7f7c39-5a01-4bd4-9a93-e3120136ffa7', metadata={'page': 4.0, 'source': '../data_rp/RP1.pdf'}, page_content='Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\\nSoumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep'),\n",
       " Document(id='2c3ae5de-4207-4b9e-8970-507af14a5707', metadata={'page': 4.0, 'source': '../data_rp/RP1.pdf'}, page_content='Chintala 2019 PyTorch An Imperative Style HighPerformance Deep Learning Library In Advances Neural Information Processing Systems 32 H Wallach H Larochelle A Beygelzimer F dAlchBuc E Fox R Garnett Eds Curran Associates Inc 80248035 httppapersneuripsccpaper'),\n",
       " Document(id='c6a769c6-0c40-4f2c-92c5-bcd48479eca9', metadata={'page': 0.0, 'source': '../data_rp/RP1.pdf'}, page_content='To accommodate different research needs, we select two deep\\nlearning frameworks for Tevatron, Pytorch [21] and JAX[4]. Py-\\ntorch’s eager execution patterns and intuitive object-oriented de-\\nsign have gained its massive user base in the research community.'),\n",
       " Document(id='91ce460e-2708-4168-adfd-5cb497e9f1f7', metadata={'page': 3.0, 'source': '../data_rp/RP1.pdf'}, page_content='on a machine with NVIDIA A100 GPUs. In both DPR-repo and\\nTevatron-default settings, we train the dense retriever model on 4\\nGPUs in distributed data-parallel mode of Pytorch. By comparing\\nthe first two rows in Table 2, we see that Tevatron is more efficient')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "retrieved_docs = retriever.invoke(\"Is it compatible with Pytorch?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\",\n",
    "                             temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tevatron is a Python module available on the Python Package Index and can be installed via pip.  It supports the full pipeline of data preparation, training, encoding, and search for dense retrieval.  More information can be found at tevatron.ai.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Tevatron?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tevatron is a Python module available on the Python Package Index.  It can be installed via pip: `pip install tevatron==0.1.0`.  The toolkit has a modularized design and command-line interfaces for development.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"How to use the toolkit?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
