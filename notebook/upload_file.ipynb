{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589d0322b6cb778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:56:47.475513Z",
     "start_time": "2025-05-03T22:56:47.466045Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AjaysPC1/Documents/research_paper_RAG_chain/rp_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468d9653a5c2c746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:56:48.904442Z",
     "start_time": "2025-05-03T22:56:48.785869Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/ccrg_j914ln8wydhc308crs00000gp/T/ipykernel_69805/3588197758.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY2')\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"research-paper-llm-db5\"\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772474c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12635683499e8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:52:21.893873Z",
     "start_time": "2025-05-03T22:52:21.884072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract Data From a Single PDF File\n",
    "def load_pdf_file(file_path: str):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    document = loader.load()\n",
    "    return document\n",
    "\n",
    "# Extract Data From Directory\n",
    "def load_pdfs_from_directory(directory_path: str):\n",
    "    loader= DirectoryLoader(directory_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by:\n",
    "    - Removing non-alphanumeric characters (except spaces).\n",
    "    - Normalizing whitespace.\n",
    "    - Removing headers, footers, or page numbers using regex patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r\"Page \\d+|Header Text|Footer Text\", \"\", text) # Remove page num, headers/footers\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, stop_words)) + r')\\b'\n",
    "    result = re.sub(pattern, '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Converts document pages to a dictionary with source(pdf file path) as key\n",
    "def group_docs_by_path(documents):\n",
    "    grouped_docs = defaultdict(list)\n",
    "    final_grouped_docs = defaultdict(list)\n",
    "    for doc in documents:\n",
    "        source = doc.metadata['source'] # fallback in case source is missing\n",
    "        page = {\n",
    "            'title': doc.metadata.get('title', None),\n",
    "            'total_pages': doc.metadata.get('total_pages', None),\n",
    "            'page': doc.metadata.get('page', None),\n",
    "            'page_label': doc.metadata.get('page_label', None),\n",
    "            'page_content': doc.page_content\n",
    "        }\n",
    "        grouped_docs[source].append(page)\n",
    "\n",
    "    for source, pages in grouped_docs.items():\n",
    "        if len(pages) == 0:\n",
    "            continue\n",
    "        page_content = ''.join([page['page_content'] for page in pages])\n",
    "        cleaned_page_content = clean_text(page_content)\n",
    "        final_grouped_docs[source] = {\n",
    "            'title': pages[0]['title'],\n",
    "            'total_pages': pages[0]['total_pages'],\n",
    "            'page_content' : page_content,\n",
    "            'cleaned_page_content': cleaned_page_content\n",
    "        }\n",
    "\n",
    "    return final_grouped_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de40d40d880171a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:52:23.094503Z",
     "start_time": "2025-05-03T22:52:23.084803Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_by_section(text):\n",
    "    '''\n",
    "        Splits the text into sections based on common section headers.\n",
    "    '''\n",
    "    pattern = r\"\\n\\d{0,2}\\.?\\s*(abstract|introduction|toolkit overview|toolkit usage|related work|experiments|methodology|results?|conclusion|references)\\s*\\n\"\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "\n",
    "    sections = []\n",
    "    for i in range(len(matches)):\n",
    "        title = matches[i].group().strip()\n",
    "        start = matches[i].start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        section_text = text[start:end].strip()\n",
    "        sections.append((title, section_text))\n",
    "    return sections\n",
    "\n",
    "\n",
    "def chunk_section(section_text, section_title, doc_title, source, chunk_size=1000, chunk_overlap=40):\n",
    "    \"\"\"\n",
    "        Splits the section text into smaller chunks.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(section_text)\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                'section': section_title,\n",
    "                'title': doc_title,\n",
    "                'source': source\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunk_all_sections(doc_map):\n",
    "    for source, doc in doc_map.items():\n",
    "        sections = split_by_section(doc['page_content'])\n",
    "        all_chunks = []\n",
    "        for title, content in sections:\n",
    "            section_title = title.strip().lower().replace(\"\\n\", \" \").strip()\n",
    "            section_chunks = chunk_section(content, section_title, doc['title'], source)\n",
    "            all_chunks.extend(section_chunks)\n",
    "        doc_map[source]['chunks'] = all_chunks\n",
    "        doc_map[source]['sections'] = sections\n",
    "\n",
    "    return doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb98e88a3b440f37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:53:56.944294Z",
     "start_time": "2025-05-03T22:53:56.937119Z"
    }
   },
   "outputs": [],
   "source": [
    "def push_to_vector_db(document_map):\n",
    "    for doc in document_map.values():\n",
    "        if 'chunks' not in doc:\n",
    "            continue\n",
    "        text_chunks = doc['chunks']\n",
    "        docsearch = PineconeVectorStore.from_documents(\n",
    "        documents=text_chunks,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab6df83787a7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be used during initialization\n",
    "documents = load_pdfs_from_directory(directory_path='../data_rp/')\n",
    "\n",
    "# To be used to upload a new file\n",
    "# extracted_data = load_pdf_file(file_path='../data_rp/RP1.pdf')\n",
    "\n",
    "\n",
    "doc_map = group_docs_by_path(documents)\n",
    "doc_map = chunk_all_sections(doc_map)\n",
    "push_to_vector_db(doc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6bb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
